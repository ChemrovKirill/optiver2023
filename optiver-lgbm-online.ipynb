{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":5505.464735,"end_time":"2023-11-05T13:50:51.295602","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-05T12:19:05.830867","version":"2.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":57891,"databundleVersionId":7056235},{"sourceType":"datasetVersion","sourceId":7242286,"datasetId":4181331,"databundleVersionId":7331970}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ğŸ§¹ Importing necessary libraries","metadata":{"papermill":{"duration":0.007624,"end_time":"2023-11-05T12:19:09.308396","exception":false,"start_time":"2023-11-05T12:19:09.300772","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Structured Approach to Advanced Predictions ğŸ“˜ğŸ“\nWe adhere to a systematic and analytical approach:\n1. **Data Preparation**: Stringent preprocessing for a robust modeling foundation.\n2. **Feature Engineering**: Deliberate crafting of features that reflect the dynamics of the market.\n3. **Purging Cross-Validation**: Implementing an innovative CV strategy that honors the temporal aspect of financial data.\n4. **Model Training**: Concentrating on models that excel in precision and generalizability.\n5. **Prediction & Strategy**: Crafting a well-thought-out plan for prediction and competition submission.","metadata":{"papermill":{"duration":0.006865,"end_time":"2023-11-05T12:19:09.322366","exception":false,"start_time":"2023-11-05T12:19:09.315501","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# ğŸ“¦ Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\nfrom catboost import CatBoostRegressor, Pool\nfrom catboost import EShapCalcType, EFeaturesSelectionAlgorithm\n\nfrom tqdm.notebook import tqdm  # Progress bar\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numba\n\n# ğŸ¤ Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# ğŸ“Š Define flags and variables\nis_offline = False  # Flag for online/offline mode\nis_train = False  # Flag for training mode\nis_infer = True  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\n\nDATE_NUM = 481\nSEC_NUM = 55\nSTOCK_NUM = 200\nLAST_SEC = 540","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.118103,"end_time":"2023-11-05T12:19:14.447911","exception":false,"start_time":"2023-11-05T12:19:09.329808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:21:39.707091Z","iopub.execute_input":"2023-12-20T18:21:39.707429Z","iopub.status.idle":"2023-12-20T18:21:45.680692Z","shell.execute_reply.started":"2023-12-20T18:21:39.707401Z","shell.execute_reply":"2023-12-20T18:21:45.679413Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## ğŸ“Š Data Loading and Preprocessing ğŸ“Š\n\n\n\n\n","metadata":{"papermill":{"duration":0.007122,"end_time":"2023-11-05T12:19:14.462224","exception":false,"start_time":"2023-11-05T12:19:14.455102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf_shape = df.shape","metadata":{"papermill":{"duration":18.240502,"end_time":"2023-11-05T12:19:32.72328","exception":false,"start_time":"2023-11-05T12:19:14.482778","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:21:45.682723Z","iopub.execute_input":"2023-12-20T18:21:45.683022Z","iopub.status.idle":"2023-12-20T18:22:04.496042Z","shell.execute_reply.started":"2023-12-20T18:21:45.682995Z","shell.execute_reply":"2023-12-20T18:22:04.495229Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Missed Stock_ids","metadata":{}},{"cell_type":"code","source":"def fill_missed_rows(df, mode='per_date'):\n    assert 'row_id' in df.columns\n    row_ids = set(df['row_id'].values)\n    all_row_ids = set()\n    if mode == 'per_second':  # ensures only all stocks per second\n        seconds = df['seconds_in_bucket'].unique()\n    elif mode == 'per_date':  # ensures all stocks and seconds per date\n        seconds = range(0, 541, 10)\n    # find missed rows\n    for d in df['date_id'].astype(int).unique():\n        for s in range(STOCK_NUM):\n            for t in seconds:\n                all_row_ids.add(f'{d}_{t}_{s}')\n    rows_missed = list(all_row_ids - row_ids)\n    \n    # dataframe with messed rows\n    df_missed = pd.DataFrame({'row_id':rows_missed})\n    df_missed['date_id'] = [int(float(d)) for d, t, s in [row.split('_') for row in rows_missed]]\n    df_missed['seconds_in_bucket'] = [int(float(t)) for d, t, s in [row.split('_') for row in rows_missed]]\n    df_missed['stock_id'] = [int(float(s)) for d, t, s in [row.split('_') for row in rows_missed]]\n\n    df = pd.concat([df, df_missed])\n    df = df.sort_values(by=['date_id','seconds_in_bucket','stock_id']).reset_index(drop=True)\n    df.reset_index(drop=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:04.497183Z","iopub.execute_input":"2023-12-20T18:22:04.497502Z","iopub.status.idle":"2023-12-20T18:22:04.507824Z","shell.execute_reply.started":"2023-12-20T18:22:04.497451Z","shell.execute_reply":"2023-12-20T18:22:04.506663Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## ğŸš€ Memory Optimization Function with Data Type Conversion ğŸ§¹","metadata":{"papermill":{"duration":0.006627,"end_time":"2023-11-05T12:19:32.737143","exception":false,"start_time":"2023-11-05T12:19:32.730516","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ğŸ§¹ Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    if verbose:\n        start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                df[col] = df[col].astype(np.float32)\n\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n        \n    return df\n","metadata":{"papermill":{"duration":0.139016,"end_time":"2023-11-05T12:19:32.896481","exception":false,"start_time":"2023-11-05T12:19:32.757465","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:22:04.510053Z","iopub.execute_input":"2023-12-20T18:22:04.510353Z","iopub.status.idle":"2023-12-20T18:22:04.525581Z","shell.execute_reply.started":"2023-12-20T18:22:04.510326Z","shell.execute_reply":"2023-12-20T18:22:04.524700Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":" ## ğŸï¸Parallel Triplet Imbalance Calculation with Numba","metadata":{"papermill":{"duration":0.006686,"end_time":"2023-11-05T12:19:32.910838","exception":false,"start_time":"2023-11-05T12:19:32.904152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    return imbalance_features\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"papermill":{"duration":0.666737,"end_time":"2023-11-05T12:19:33.59774","exception":false,"start_time":"2023-11-05T12:19:32.931003","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:22:04.526720Z","iopub.execute_input":"2023-12-20T18:22:04.527012Z","iopub.status.idle":"2023-12-20T18:22:04.640481Z","shell.execute_reply.started":"2023-12-20T18:22:04.526986Z","shell.execute_reply":"2023-12-20T18:22:04.639638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Lag-Depended Functions","metadata":{}},{"cell_type":"code","source":"def add_lag_lastsec(df, col_name, lag, func):\n    ''' ASSUMPTION: dates are consistent!!! '''\n    col_lag = [f'{col}_lastsec_lag_{lag}' for col in col_name]\n    date_ids = df['date_id']\n    min_date = date_ids.min()\n    max_date = date_ids.max()\n    if max_date - min_date < lag:\n        df[col_lag] = np.nan\n        return df\n    for group_id, values in df.groupby(['date_id'])[col_name]:\n        curr_date = int(np.array([group_id]).flatten()[0])\n        lag_values = df[(df['date_id'] == curr_date - lag) & (df['seconds_in_bucket'] == LAST_SEC)][col_name]\n        if not lag_values.empty:\n            df.loc[values.index, col_lag] = np.tile(lag_values, (len(values)//len(lag_values),1))\n        else:\n            df.loc[values.index, col_lag] = np.nan\n    return df\n\ndef add_lag_constsec(df, col_name, lag, func):\n    ''' ASSUMPTION: dates are consistent!!! '''\n    if 'lag' in func or 'shift' in func:\n        col_lag = [f'{col}_constsec_lag_{lag}' for col in col_name]\n        lag_values = df[col_name].shift(lag * STOCK_NUM * SEC_NUM)\n        df[col_lag] = lag_values\n    if 'pct' in func or 'ret' in func:\n        col_ret = [f'{col}_constsec_ret_{lag}' for col in col_name]\n        ret_values = df[col_name].pct_change(lag * STOCK_NUM * SEC_NUM)\n        df[col_ret] = ret_values\n    if 'diff' in func :\n        col_diff = [f'{col}_constsec_diff_{lag}' for col in col_name]\n        diff_values = df[col_name].diff(lag * STOCK_NUM * SEC_NUM)\n        df[col_diff] = diff_values\n    return df\n\ndef add_lag_nolimit(df, col_name, lag, func):\n    ''' ASSUMPTION: dates are consistent!!! '''\n    if 'lag' in func or 'shift' in func:\n        col_lag = [f'{col}_nolimit_lag_{lag}' for col in col_name]\n        lag_values = df[col_name].shift(lag * STOCK_NUM)\n        df[col_lag] = lag_values\n    if 'pct' in func or 'ret' in func:\n        col_ret = [f'{col}_nolimit_ret_{lag}' for col in col_name]\n        ret_values = df[col_name].pct_change(lag * STOCK_NUM)\n        df[col_ret] = ret_values\n    if 'diff' in func :\n        col_diff = [f'{col}_nolimit_diff_{lag}' for col in col_name]\n        diff_values = df[col_name].diff(lag * STOCK_NUM)\n        df[col_diff] = diff_values\n    return df\n\ndef add_lag_intraday(df, col_name, lag, func):\n    ''' ASSUMPTION: dates are consistent!!! '''\n    if 'lag' in func or 'shift' in func:\n        col_lag = [f'{col}_intraday_lag_{lag}' for col in col_name]\n        lag_values = df[col_name].shift(lag * STOCK_NUM)\n        df[col_lag] = lag_values\n        df.loc[df['seconds_in_bucket'] < lag*10, col_lag] = np.nan\n    if 'pct' in func or 'ret' in func:\n        col_ret = [f'{col}_intraday_ret_{lag}' for col in col_name]\n        ret_values = df[col_name].pct_change(lag * STOCK_NUM)\n        df[col_ret] = ret_values\n        df.loc[df['seconds_in_bucket'] < lag*10, col_ret] = np.nan\n    if 'diff' in func :\n        col_diff = [f'{col}_intraday_diff_{lag}' for col in col_name]\n        diff_values = df[col_name].diff(lag * STOCK_NUM)\n        df[col_diff] = diff_values\n        df.loc[df['seconds_in_bucket'] < lag*10, col_diff] = np.nan\n    return df\n\ndef generate_lag_features(df, col_name, lags, func, lag_type='nolimit'):\n    \"\"\"\n    Adds Lag-depended features.\n    \n    Parameters:\n    df (pandas.DataFrame): input DataFrame.\n    col_name (str): column for lag.\n    lags (list): lag values for funcs.\n    func (str): lag-dependent function: lag, ret, diff, roll\n    lag_type (str): type of information access method: intraday, constsec, nolimit, lastsec (lag_type=lastsec is compatible with func=lag only!).\n    \n    Returns:\n    pandas.DataFrame: result DataFrame\n    \"\"\"\n    lag_type_funcs = {'intraday': add_lag_intraday, \n                     'constsec': add_lag_constsec, \n                     'nolimit': add_lag_nolimit,\n                     'lastsec': add_lag_lastsec}\n    lag_type_func = lag_type_funcs[lag_type]\n       \n    for lag in lags:\n        df = lag_type_func(df, col_name, lag, func)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:04.641863Z","iopub.execute_input":"2023-12-20T18:22:04.642163Z","iopub.status.idle":"2023-12-20T18:22:04.663108Z","shell.execute_reply.started":"2023-12-20T18:22:04.642137Z","shell.execute_reply":"2023-12-20T18:22:04.662013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Mean by Day / Second","metadata":{}},{"cell_type":"code","source":"@numba.njit(\"float32[:](float32[:], int64)\", parallel=True, nogil=True)\ndef mean_by_period_jit(x, w):\n    y = np.empty_like(x)\n    iters = len(x) // w\n    for i in numba.prange(iters):\n        y[i*w: (i+1)*w] = np.nanmean(x[i*w: (i+1)*w])\n    return y\n\ndef mean_by_period(df, col_name, period='sec'):\n    w = STOCK_NUM\n    if period == 'day':\n        w = STOCK_NUM * SEC_NUM\n    \n    for col in col_name:\n        mean_col = f'{col}_mean'\n        x = df[col].values\n        df[mean_col] = mean_by_period_jit(x, w)\n        \n    return df\n\ndef mean_and_diff_by_period(df, col_name, period='sec'):\n    w = STOCK_NUM\n    if period == 'day':\n        w = STOCK_NUM * SEC_NUM\n    \n    for col in col_name:\n        mean_col = f'{col}_mean'\n        mean_diff_col = f'{col}_mean_diff'\n        x = df[col].values\n        df[mean_col] = mean_by_period_jit(x, w)\n        df[mean_diff_col] = df[col] - df[mean_col]\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:04.664661Z","iopub.execute_input":"2023-12-20T18:22:04.664949Z","iopub.status.idle":"2023-12-20T18:22:06.285345Z","shell.execute_reply.started":"2023-12-20T18:22:04.664923Z","shell.execute_reply":"2023-12-20T18:22:06.284481Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“Š Feature Generation Functions ğŸ“Š\n\n\n\n\n","metadata":{"papermill":{"duration":0.006707,"end_time":"2023-11-05T12:19:33.611671","exception":false,"start_time":"2023-11-05T12:19:33.604964","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ğŸ“Š Function to generate imbalance features\ndef imbalance_features(df, log=False):\n    start_time = time.time()\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size_signed\"]\n\n    # V1: syntetic\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff(periods=1)  # df = generate_lag_features(df, col_name='price_spread', lags=[1], func=['diff'], lag_type='intraday')\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n#     df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"])\n    df['price_diff'] = df['reference_price'] - df['wap']\n    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n    df['reference_price_wap_imb'] = df.eval(\"(reference_price-wap) / (reference_price+wap)\")\n    df['reference_price_bid_price_imb'] = df.eval(\"(reference_price-bid_price) / (reference_price+bid_price)\")   \n    df['far_price_near_price_imb'] = df.eval(\"(far_price - near_price) / (far_price + near_price)\")   \n    df['far_price_over_near_price'] = df[\"far_price\"].div(df[\"near_price\"])\n    df['far_price_minus_near_price'] = df[\"far_price\"] - df[\"near_price\"]\n    \n    if log:\n        print('Created: V1 |', time.time() - start_time)\n        start_time = time.time()\n    \n    # V2: imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n    if log:\n        print('Created: V2 |', (time.time() - start_time))   \n        start_time = time.time()\n    \n    # V3: lag features\n    col_names = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"] + sizes\n    df = generate_lag_features(df, col_name=col_names, lags=[1, 2, 5, 10, 15], func=['lag', 'ret'], lag_type='nolimit')\n    df = generate_lag_features(df, col_name=prices, lags=[1, 2, 5, 10, 15], func=['diff'], lag_type='nolimit')\n    if log:\n        print('Created: V3 |', time.time() - start_time)\n        start_time = time.time()\n        \n    # V4: lag target\n    df = generate_lag_features(df, col_name=['target'], lags=[1, 2, 3, 4], func=['lag'], lag_type='constsec')\n    df = generate_lag_features(df, col_name=['target'], lags=[1, 2, 3, 4], func=['lag'], lag_type='lastsec')\n    if log:\n        print('Created: V4 |', time.time() - start_time)\n        start_time = time.time()\n        \n    # V5: mean features + meandiffs\n    price_mean_col_names = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\",\n                            \"reference_price_nolimit_ret_1\", \"ask_price_nolimit_ret_1\", \"bid_price_nolimit_ret_1\", \"wap_nolimit_ret_1\",\n                            \"reference_price_nolimit_diff_1\", \"ask_price_nolimit_diff_1\", \"bid_price_nolimit_diff_1\", \"wap_nolimit_diff_1\",]\n    size_mean_col_names = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size_signed\"]\n#                            \"matched_size_nolimit_ret_1\", \"bid_size_nolimit_ret_1\", \"ask_size_nolimit_ret_1\", \"imbalance_size_signed_nolimit_ret_1\",\n#                            \"matched_size_nolimit_diff_1\", \"bid_size_nolimit_diff_1\", \"ask_size_nolimit_diff_1\", \"imbalance_size_signed_nolimit_diff_1\"]\n    df = mean_and_diff_by_period(df, col_name=price_mean_col_names, period='sec')\n    df = mean_and_diff_by_period(df, col_name=size_mean_col_names, period='sec')\n    \n    reduced_prices = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sma_col_names = [\"sma_10_reference_price\", \"sma_10_ask_price\", \"sma_10_bid_price\", \"sma_10_wap\"]\n    sma_diff_col_names = [\"sma_10_reference_price_currdiff\", \"sma_10_ask_price_currdiff\", \"sma_10_bid_price_currdiff\", \"sma_10_wap_currdiff\"]\n    for i in range(len(reduced_prices)):\n        df[sma_diff_col_names[i]] = df[reduced_prices[i]] - df[sma_col_names[i]]\n    if log:\n        print('Created: V5 |', time.time() - start_time)\n\n    return df.replace([np.inf, -np.inf], 0)","metadata":{"papermill":{"duration":0.029023,"end_time":"2023-11-05T12:19:33.660873","exception":false,"start_time":"2023-11-05T12:19:33.63185","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:22:06.286558Z","iopub.execute_input":"2023-12-20T18:22:06.286844Z","iopub.status.idle":"2023-12-20T18:22:06.309201Z","shell.execute_reply.started":"2023-12-20T18:22:06.286819Z","shell.execute_reply":"2023-12-20T18:22:06.308094Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Technical Analysis Indicators","metadata":{}},{"cell_type":"markdown","source":"### 0. SMA & EMA","metadata":{}},{"cell_type":"code","source":"@numba.njit(\"float32[:](float32[:], int64)\", nogil=True)\ndef sma1d(x, w):\n    y = np.empty_like(x)\n    asum = 0.0\n    for i in range(w):\n        asum += x[i]\n        y[i] = asum / w\n    for i in range(w, len(x)):\n        asum += x[i] - x[i - w]\n        y[i] = asum / w\n    return y\n\ndef sma2d(x, w):  # 144 ms Â± 10.8 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n    y = np.empty_like(x)\n    for i in range(x.shape[1]):\n        y[:, i] = sma1d(x[:, i], w)\n    return y\n\n@numba.njit(\"float32[:,:](float32[:,:], int64)\", nogil=True, parallel=True)\ndef sma2d_jit(x, w):  # 203 ms Â± 3.51 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n    y = np.empty_like(x)\n    for i in numba.prange(x.shape[1]):\n        y[:, i] = sma1d(x[:, i], w)\n    return y","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:06.310291Z","iopub.execute_input":"2023-12-20T18:22:06.310603Z","iopub.status.idle":"2023-12-20T18:22:07.241768Z","shell.execute_reply.started":"2023-12-20T18:22:06.310568Z","shell.execute_reply":"2023-12-20T18:22:07.240762Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def calculate_sma_jit(data, period=14):\n    init_shape = data.shape\n    data = data.reshape((-1, STOCK_NUM*init_shape[1]))\n    sma = sma2d_jit(data, period)\n    return sma.reshape(init_shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:07.245351Z","iopub.execute_input":"2023-12-20T18:22:07.245750Z","iopub.status.idle":"2023-12-20T18:22:07.250823Z","shell.execute_reply.started":"2023-12-20T18:22:07.245724Z","shell.execute_reply":"2023-12-20T18:22:07.249842Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@numba.njit(\"float32[:](float32[:], int64)\", nogil=True)\ndef ema1d(x, span):\n    a = 2 / (1 + span)\n    y = np.empty_like(x)\n    y[0] = x[0]\n    for k in range(1, len(y)):\n        y[k] = y[k-1]*(1 - a) + x[k]*a\n    return y\n\ndef ema2d(x, span):  # 309 ms Â± 485 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n    y = np.empty_like(x)\n    for i in range(x.shape[1]):\n        y[:, i] = ema1d(x[:, i], span)\n    return y\n\n@numba.njit(\"float32[:,:](float32[:,:], int64)\", nogil=True, parallel=True)\ndef ema2d_jit(x, span):  # 238 ms Â± 2.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n    y = np.empty_like(x)\n    for i in numba.prange(x.shape[1]):\n        y[:, i] = ema1d(x[:, i], span)\n    return y","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:07.251965Z","iopub.execute_input":"2023-12-20T18:22:07.252295Z","iopub.status.idle":"2023-12-20T18:22:08.172522Z","shell.execute_reply.started":"2023-12-20T18:22:07.252259Z","shell.execute_reply":"2023-12-20T18:22:08.171741Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def calculate_ema_jit(data, period=14):\n    init_shape = data.shape\n    data = data.reshape((-1, STOCK_NUM*init_shape[1]))\n    ema = ema2d_jit(data, period)\n    return ema.reshape(init_shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:08.173658Z","iopub.execute_input":"2023-12-20T18:22:08.173947Z","iopub.status.idle":"2023-12-20T18:22:08.179308Z","shell.execute_reply.started":"2023-12-20T18:22:08.173922Z","shell.execute_reply":"2023-12-20T18:22:08.178259Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 1. Relative Strength Index (RSI)\n\n$$\n    \\text{RSI} = 100 - \\frac{100}{1 + \\text{RS}}\n$$\n\nWhere: \n$$\nRS = (Average Gain over N days) / (Average Loss over N days)\n$$","metadata":{}},{"cell_type":"code","source":"def calculate_rsi_jit(data, period=14):\n    init_shape = data.shape\n    data = data.reshape((-1, STOCK_NUM*init_shape[1]))\n\n    delta = np.zeros_like(data)\n    delta[1:] = data[1:] - data[:-1]\n\n    gain = np.where(delta > 0, delta, 0)\n    loss = np.where(delta < 0, -delta, 0)\n\n    rs = ema2d_jit(gain, period) / ema2d_jit(loss, period)\n    rsi = 100 - (100 / (1 + rs))\n    return rsi.reshape(init_shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:08.180538Z","iopub.execute_input":"2023-12-20T18:22:08.180797Z","iopub.status.idle":"2023-12-20T18:22:08.191228Z","shell.execute_reply.started":"2023-12-20T18:22:08.180774Z","shell.execute_reply":"2023-12-20T18:22:08.190314Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### 2. MACD\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\text{MACD} & = \\text{EMA}(12) - \\text{EMA}(26) \\\\\n        \\text{Signal Line} & = \\text{EMA}(\\text{MACD}, 9) \\\\\n        \\text{Histogram} & = \\text{MACD} - \\text{Signal Line}\n    \\end{aligned}\n\\end{equation}\n$$","metadata":{}},{"cell_type":"code","source":"def calculate_macd_jit(data, short_window=12, long_window=26, signal_window=9):\n    init_shape = data.shape\n    data = data.reshape(-1, STOCK_NUM*init_shape[1])\n    \n    short_ema = ema2d_jit(data, short_window)\n    long_ema = ema2d_jit(data, long_window)\n    macd = short_ema - long_ema\n    \n    signal = ema2d_jit(macd, signal_window)\n\n    histogram = macd - signal\n\n    return macd.reshape(init_shape), signal.reshape(init_shape), histogram.reshape(init_shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:08.192488Z","iopub.execute_input":"2023-12-20T18:22:08.192812Z","iopub.status.idle":"2023-12-20T18:22:08.204517Z","shell.execute_reply.started":"2023-12-20T18:22:08.192782Z","shell.execute_reply":"2023-12-20T18:22:08.203740Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 3. Bollinger Bands\n\n$$ BBands = SMA(W) \\pm 2std $$","metadata":{}},{"cell_type":"code","source":"@numba.njit(\"float32(float32[:])\", nogil=True)\ndef std_jit(x):\n    x_mean = 0.0\n    for i in range(len(x)):\n        x_mean += x[i]\n    x_mean = x_mean / len(x)\n    x_delta_2 = (x - x_mean)**2\n    x_delta_2_mean = 0.0\n    for i in range(len(x_delta_2)):\n        x_delta_2_mean += x_delta_2[i]\n    x_delta_2_mean /= len(x_delta_2)\n    std = np.sqrt(x_delta_2_mean)\n    return std\n\n@numba.njit(\"UniTuple(float32[:], 2)(float32[:], int64)\", nogil=True)\ndef sma1d_w_std(x, w):\n    y = np.empty_like(x)\n    stds = np.empty_like(x)\n    asum = 0.0\n    for i in range(w):\n        asum += x[i]\n        y[i] = asum / w\n        stds[i] = std_jit(x[:i+1])\n    for i in range(w, len(x)):\n        asum += x[i] - x[i - w]\n        y[i] = asum / w\n        stds[i] = std_jit(x[i-w+1: i+1])\n    return y, stds\n\n@numba.njit(\"UniTuple(float32[:,:], 3)(float32[:,:], int64, int64)\", nogil=True, parallel=True)\ndef bband2d_jit(x, w, num_std_dev):  # 203 ms Â± 3.51 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n    mid_bands = np.empty_like(x)\n    upper_bands = np.empty_like(x)\n    lower_bands = np.empty_like(x)\n    for i in numba.prange(x.shape[1]):\n        mid_bands[:, i], std_dev = sma1d_w_std(x[:, i], w)\n        upper_bands[:, i] = mid_bands[:, i] + std_dev * num_std_dev\n        lower_bands[:, i] = mid_bands[:, i] - std_dev * num_std_dev\n    return upper_bands, mid_bands, lower_bands","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:08.205711Z","iopub.execute_input":"2023-12-20T18:22:08.206105Z","iopub.status.idle":"2023-12-20T18:22:10.251927Z","shell.execute_reply.started":"2023-12-20T18:22:08.206064Z","shell.execute_reply":"2023-12-20T18:22:10.251123Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def calculate_bband_jit(data, window=20, num_std_dev=2): #  1.54 s Â± 75 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)   (vs 2.19 s via Pandas)\n    init_shape = data.shape\n    data = data.reshape(-1, STOCK_NUM*init_shape[1])\n    upper_bands, mid_bands, lower_bands = bband2d_jit(data, window, num_std_dev)\n    return upper_bands.reshape(init_shape), mid_bands.reshape(init_shape), lower_bands.reshape(init_shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.253030Z","iopub.execute_input":"2023-12-20T18:22:10.253340Z","iopub.status.idle":"2023-12-20T18:22:10.259245Z","shell.execute_reply.started":"2023-12-20T18:22:10.253315Z","shell.execute_reply":"2023-12-20T18:22:10.258257Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### TA Indicators Compilation","metadata":{}},{"cell_type":"code","source":"def generate_ta(df, lag_type='intraday'):\n    \"\"\"\n    Generate TA features.\n    \n    Parameters:\n    df (pandas.DataFrame): input DataFrame.\n    lag_type (str): type of information access method: intraday, nolimit. //constsec is not realized\n    \n    Returns:\n    pandas.DataFrame: result DataFrame\n    \"\"\"\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    # sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    lag_sma = [2, 5, 10, 15]\n    lag_rsi = 14\n    lag_macd_short = 20\n    lag_macd_long = 26\n    lag_macd_sig = 9\n    lag_bband = 20\n    \n    values = df[prices].values.astype(np.float32)\n    \n    # fast SMA\n    for lag in lag_sma:\n        col_sma = [f'sma_{lag}_{col}' for col in prices]\n        sma_values = calculate_sma_jit(values, lag)\n        df.loc[:, col_sma] = sma_values\n    \n    # fast RSI\n    col_rsi = [f'rsi_{col}' for col in prices]\n    rsi_values = calculate_rsi_jit(values, period=lag_rsi)\n    df.loc[:, col_rsi] = rsi_values\n    \n    # fast MACD\n    macd, signal, histogram = calculate_macd_jit(values, \n                                             short_window=lag_macd_short, \n                                             long_window=lag_macd_long, \n                                             signal_window=lag_macd_sig)\n    col_macd = [f'macd_{col}' for col in prices]\n    col_signal = [f'macd_sig_{col}' for col in prices]\n    col_hist = [f'macd_hist_{col}' for col in prices]\n\n    df.loc[:, col_macd] = macd\n    df.loc[:, col_signal] = signal\n    df.loc[:, col_hist] = histogram\n\n    # fast Bollinger Bands\n    bband_upper_values, bband_mid_values, bband_lower_values = calculate_bband_jit(values, window=lag_bband, num_std_dev=2)\n    col_bband_upper = [f'bband_upper_{col}' for col in prices]\n    col_bband_mid = [f'bband_mid_{col}' for col in prices]\n    col_bband_lower = [f'bband_lower_{col}' for col in prices]\n\n    df.loc[:, col_bband_upper] = bband_upper_values\n    df.loc[:, col_bband_mid] = bband_mid_values\n    df.loc[:, col_bband_lower] = bband_lower_values\n        \n    if lag_type == 'intraday':\n        df.loc[df['seconds_in_bucket'] < lag_rsi * 10, col_rsi] = np.nan\n        df.loc[df['seconds_in_bucket'] < lag_macd_long * 10, col_macd + col_signal + col_hist] = np.nan\n        df.loc[df['seconds_in_bucket'] < lag_bband * 10, col_bband_upper + col_bband_mid + col_bband_lower] = np.nan\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.260491Z","iopub.execute_input":"2023-12-20T18:22:10.260761Z","iopub.status.idle":"2023-12-20T18:22:10.273850Z","shell.execute_reply.started":"2023-12-20T18:22:10.260737Z","shell.execute_reply":"2023-12-20T18:22:10.273019Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### All Features Compilation","metadata":{}},{"cell_type":"code","source":"def handlenan(df, test=False):\n    columns_to_drop = ['target']\n    columns_to_fill = list(set(df.columns) - set(columns_to_drop))\n    if not test:\n        df.dropna(subset=columns_to_drop, inplace=True)\n        df.fillna(0, inplace=True)\n    else:\n        df[columns_to_fill] = df[columns_to_fill].fillna(0)\n    df.replace([np.inf, -np.inf], 0, inplace=True)\n    df.reset_index(drop=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.274948Z","iopub.execute_input":"2023-12-20T18:22:10.275281Z","iopub.status.idle":"2023-12-20T18:22:10.288776Z","shell.execute_reply.started":"2023-12-20T18:22:10.275254Z","shell.execute_reply":"2023-12-20T18:22:10.288002Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# ğŸš€ Function to generate all features by combining imbalance and other features\ndef generate_all_features(df, test=False, log=False):\n    # Select relevant columns for feature generation\n    df[['far_price', 'near_price']] = df[['far_price', 'near_price']].fillna(0)\n    df['imbalance_size_signed'] = df['imbalance_size'] * df['imbalance_buy_sell_flag']\n    \n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"imbalance_buy_sell_flag\"]]\n    df = df[cols]\n    \n    # Generate TA features\n    if log:\n        print('Generating TA features...')\n        start = time.time()\n    df = generate_ta(df, lag_type='nolimit')\n#     if not test:\n    df = reduce_mem_usage(df)\n    if log:\n        print('Duration: ', (time.time() - start), '[s]\\n')\n    \n    # Generate imbalance and other features\n    if log:\n        print('Generating Imbalance features...')\n        start = time.time()\n    df = imbalance_features(df, log)\n    if not test:\n        df = reduce_mem_usage(df)\n    if log:\n        print('Duration: ', time.time() - start, '[s]\\n')\n        print('Generating Other features...')\n        start = time.time()\n#     df = other_features(df)\n    if log:\n        print('Duration: ', time.time() - start, '[s]')\n    if not test:\n        df = reduce_mem_usage(df)\n    if log:\n        print('Garbage collecteing...')   \n    gc.collect()  # Perform garbage collection to free up memory\n    if log:\n        print('Nan handling...')\n    df = handlenan(df, test)\n    if log:\n        print('done')\n        \n    df = reduce_mem_usage(df)\n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\", \"imbalance_size\"]]\n    \n    return df[feature_name], df[[\"target\", \"date_id\"]]","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.289826Z","iopub.execute_input":"2023-12-20T18:22:10.290696Z","iopub.status.idle":"2023-12-20T18:22:10.302195Z","shell.execute_reply.started":"2023-12-20T18:22:10.290666Z","shell.execute_reply":"2023-12-20T18:22:10.301381Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting","metadata":{"papermill":{"duration":0.006675,"end_time":"2023-11-05T12:19:33.704872","exception":false,"start_time":"2023-11-05T12:19:33.698197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    \n    # Display a message indicating offline mode and the shapes of the training and validation sets\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    print(\"Online mode\")\ndel df","metadata":{"papermill":{"duration":0.016167,"end_time":"2023-11-05T12:19:33.741631","exception":false,"start_time":"2023-11-05T12:19:33.725464","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:22:10.303223Z","iopub.execute_input":"2023-12-20T18:22:10.303514Z","iopub.status.idle":"2023-12-20T18:22:10.316088Z","shell.execute_reply.started":"2023-12-20T18:22:10.303483Z","shell.execute_reply":"2023-12-20T18:22:10.315244Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Online mode\n","output_type":"stream"}]},{"cell_type":"code","source":"if is_train:\n    print(\"Filling missed rows...\")\n    df_train = fill_missed_rows(df_train)\n    if is_offline:\n        X_train, y_train = generate_all_features(df_train, log=True)\n        print(\"Build Train Feats Finished.\")\n        df_valid = fill_missed_rows(df_valid)\n        X_valid, y_valid = generate_all_features(df_valid, log=True)\n        print(\"Build Valid Feats Finished.\")\n        X_valid = reduce_mem_usage(X_valid)\n        date_ids_valid = y_valid['date_id'].values\n        y_valid = y_valid['target']\n        del df_valid\n    else:\n        X_train, y_train = generate_all_features(df_train, log=True)\n        print(\"Build Online Train Feats Finished.\")\n    del df_train\n        \n    X_train = reduce_mem_usage(X_train)\n    date_ids = y_train['date_id'].values\n    y_train = y_train['target']\n    gc.collect()\n    global_X_shape = X_train.shape\n    print(X_train.shape, y_train.shape)\n","metadata":{"papermill":{"duration":59.048473,"end_time":"2023-11-05T12:20:32.810127","exception":false,"start_time":"2023-11-05T12:19:33.761654","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-20T18:22:10.317320Z","iopub.execute_input":"2023-12-20T18:22:10.317686Z","iopub.status.idle":"2023-12-20T18:22:10.330745Z","shell.execute_reply.started":"2023-12-20T18:22:10.317654Z","shell.execute_reply":"2023-12-20T18:22:10.329865Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"feats_num = 256\nmodel_load_dir = f'/kaggle/input/lgbm-{feats_num}feats-mean/'\nmodel_save_path = f'lgbm-{feats_num}feats-mean'  # Directory to save models\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\nmodels = [] ","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.331873Z","iopub.execute_input":"2023-12-20T18:22:10.332150Z","iopub.status.idle":"2023-12-20T18:22:10.341145Z","shell.execute_reply.started":"2023-12-20T18:22:10.332126Z","shell.execute_reply":"2023-12-20T18:22:10.340351Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\"\"\" 5-fold lgbm \"\"\"\nnum_folds = 10\nfold_size = 480 // num_folds\ngap = 2\n\nif is_train:\n    scores = []\n\n    for i in range(num_folds-5):\n        start = i * fold_size\n        end = start + fold_size\n\n        # Define the purged set ranges\n        purged_before_start = start - gap\n        purged_before_end = start + gap\n        purged_after_start = end - gap\n        purged_after_end = end + gap\n\n        # Exclude the purged ranges from the test set\n        purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n                     ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n\n        # Define test_indices excluding the purged set\n        test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n        train_indices = ~test_indices & ~purged_set\n\n        X_fold_train = X_train[train_indices]\n        y_fold_train = y_train[train_indices]\n        X_fold_valid = X_train[test_indices]\n        y_fold_valid = y_train[test_indices]\n\n        print(f\"Fold {i+1} Model Training\")\n\n        # Set up parameters for LightGBM\n        lgb_params = {\n                \"random_state\": 42*i,\n                \"objective\": \"mae\",\n                \"n_estimators\": 4000,\n                \"num_leaves\": 256,\n                \"subsample\": 0.6,\n                \"colsample_bytree\": 0.8,\n                \"learning_rate\": 0.01,\n                'max_depth': 11,\n                \"n_jobs\": 4,\n                \"device\": \"gpu\",\n                \"verbosity\": -1,\n                \"importance_type\": \"gain\",\n                \"reg_alpha\": 0.2,\n                \"reg_lambda\": 3.25\n            }\n        \n        # Train a LightGBM model for the current fold\n        lgb_model = lgb.LGBMRegressor(**lgb_params)\n        lgb_model.fit(\n            X_fold_train,\n            y_fold_train,\n            eval_set=[(X_fold_valid, y_fold_valid)],\n            callbacks=[\n                lgb.callback.early_stopping(stopping_rounds=100),\n                lgb.callback.log_evaluation(period=100),\n            ],\n        )\n\n        # Append the model to the list\n        models.append(lgb_model)\n        # Save the model to a file\n        model_filename = os.path.join(model_save_path, f'model_10folds_{i+1}.txt')\n        lgb_model.booster_.save_model(model_filename)\n        print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n        # Evaluate model performance on the validation set\n        fold_predictions = lgb_model.predict(X_fold_valid)\n        fold_score = mean_absolute_error(fold_predictions, y_fold_valid)\n        scores.append(fold_score)\n        print(f\"Fold {i+1} MAE: {fold_score}\")\n\n        # Free up memory by deleting fold specific variables\n        del X_fold_train, y_fold_train, X_fold_valid, y_fold_valid\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.342412Z","iopub.execute_input":"2023-12-20T18:22:10.342705Z","iopub.status.idle":"2023-12-20T18:22:10.355039Z","shell.execute_reply.started":"2023-12-20T18:22:10.342681Z","shell.execute_reply":"2023-12-20T18:22:10.354191Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"if is_train and is_offline:\n    for seed in [42]:\n        print(f\"Seed = {seed}\")\n        # Set up parameters for LightGBM\n        lgb_params = {\n                \"random_state\": seed,\n                \"objective\": \"mae\",\n                \"n_estimators\": 4000,\n                \"num_leaves\": 256,\n                \"subsample\": 0.6,\n                \"colsample_bytree\": 0.8,\n                \"learning_rate\": 0.01,\n                'max_depth': 11,\n                \"n_jobs\": 4,\n                \"device\": \"gpu\",\n                \"verbosity\": -1,\n                \"importance_type\": \"gain\",\n                \"reg_alpha\": 0.2,\n                \"reg_lambda\": 3.25\n            }\n\n        # Train the model on the entire dataset\n        model = lgb.LGBMRegressor(**lgb_params)\n\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_valid, y_valid)],\n            callbacks=[\n                lgb.callback.early_stopping(stopping_rounds=100),\n                lgb.callback.log_evaluation(period=100),\n            ],\n        )\n\n        # Append the final model to the list of models\n        models.append(model)\n\n        # Save the final model to a file\n        model_filename = os.path.join(model_save_path, f'model_offline_seed{seed}_midsec.txt')\n        model.booster_.save_model(model_filename)\n        print(f\"Final model saved to {model_filename}\")\n        \n    for i, model in enumerate(models):\n        X_valid['preds'] = model.predict(X_valid)\n        X_valid['date_id'] = date_ids_valid\n        for group_id, values in X_valid.groupby(['date_id', 'seconds_in_bucket'])['preds']:\n            X_valid.loc[values.index, 'preds'] -= np.mean(values)\n        print(f\"model_{i}, MAE: {mean_absolute_error(y_valid, X_valid['preds'])}\")\nprint(f\"{len(models)} models are ready\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.356161Z","iopub.execute_input":"2023-12-20T18:22:10.356684Z","iopub.status.idle":"2023-12-20T18:22:10.370868Z","shell.execute_reply.started":"2023-12-20T18:22:10.356653Z","shell.execute_reply":"2023-12-20T18:22:10.370132Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"0 models are ready\n","output_type":"stream"}]},{"cell_type":"code","source":"seed = 42\nif not is_offline:\n    if is_train:\n        print(f\"Seed = {seed}\")\n        # Set up parameters for LightGBM\n        lgb_params = {\n                \"random_state\": seed,\n                \"objective\": \"mae\",\n                \"n_estimators\": 3000,\n                \"num_leaves\": 256,\n                \"subsample\": 0.6,\n                \"colsample_bytree\": 0.8,\n                \"learning_rate\": 0.01,\n                'max_depth': 11,\n                \"n_jobs\": 4,\n                \"device\": \"gpu\",\n                \"verbosity\": -1,\n                \"importance_type\": \"gain\",\n                \"reg_alpha\": 0.2,\n                \"reg_lambda\": 3.25\n            }\n\n        # Train the model on the entire dataset\n        model = lgb.LGBMRegressor(**lgb_params)\n\n        model.fit(\n            X_train,\n            y_train,\n            callbacks=[\n                lgb.callback.log_evaluation(period=100),\n            ],\n            # init_model=init_model\n        )\n\n        # Append the final model to the list of models\n        models.append(model)\n\n        # Save the final model to a file\n        model_filename = os.path.join(model_save_path, f'model_seed{seed}.txt')\n        model.booster_.save_model(model_filename)\n        print(f\"Final model saved to {model_filename}\")\n\n    else:  # if is not train (use saved models)\n        for i in range(5):\n            model_load_file = os.path.join(model_load_dir, f'model_10folds_{i+1}.txt')\n            models.append(lgb.Booster(model_file=model_load_file))\n        model_load_file = os.path.join(model_load_dir, f'model_seed{seed}.txt')\n        models.append(lgb.Booster(model_file=model_load_file))\n    print(f\"{len(models)} models are ready\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:10.372094Z","iopub.execute_input":"2023-12-20T18:22:10.372740Z","iopub.status.idle":"2023-12-20T18:22:17.076367Z","shell.execute_reply.started":"2023-12-20T18:22:10.372707Z","shell.execute_reply":"2023-12-20T18:22:17.075339Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"6 models are ready\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"online_train_decay = 1.\n\ndef revealed_to_filled_df(revealed):\n    df = pd.DataFrame({'date_id':revealed['revealed_date_id'],\n                       'stock_id':revealed['stock_id'],\n                       'seconds_in_bucket':revealed['seconds_in_bucket'],\n                       'target':revealed['revealed_target']})\n    df['row_id'] = [f'{d}_{t}_{s}' for d, t, s in \n                    zip(df['date_id'],df['seconds_in_bucket'],df['stock_id'])]\n    return fill_missed_rows(df)\n\n\ndef cut_tail(cache, days_to_cache):\n    cache_size = days_to_cache * STOCK_NUM * SEC_NUM\n    if len(cache) < cache_size:\n        return cache\n    cache = cache.tail(cache_size).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id'])\n    return cache.reset_index(drop=True)\n\n    \ndef sqrt_mean(preds_list):\n    preds = np.vstack(preds_list)\n    preds_sgn = np.sign(preds)\n    preds = np.sqrt(np.abs(preds))\n    preds = preds * preds_sgn\n    pred = np.mean(preds, axis=0)\n    pred_sgn = np.sign(pred)\n    pred = np.square(pred)\n    pred = pred * pred_sgn\n    return pred\n\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, gen_time,inf_time = [], [], []\n    cache = pd.DataFrame()\n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        \n        # pad rows for missed stocks\n        test['not_pad'] = [1]*len(test)\n        test = test.drop(['currently_scored'], axis=1)\n        test_filled = fill_missed_rows(test, mode='per_second')\n        \n        # save revealed targets of previous date\n        cur_sec = test['seconds_in_bucket'].values[0]\n        if cur_sec == 0:\n            revealed_df = revealed_to_filled_df(revealed_targets)\n            if counter == 0:\n                cache = revealed_df\n            else:\n                cache = fill_missed_rows(cache)  # in case of missed seconds in the previous day\n                cache = cut_tail(cache, days_to_cache=5)\n                cache.loc[cache.index[-STOCK_NUM*SEC_NUM:], 'target'] = revealed_df['target'].values\n                cache = pd.concat([cache, test_filled], ignore_index=True, axis=0)\n        cache = pd.concat([cache, test_filled], ignore_index=True, axis=0)\n        gen_start = time.time()\n        feat, target = generate_all_features(cache, test=True, log=False)\n        gen_time.append(time.time() - gen_start)\n        \n        if online_train_decay < 1.:\n            if cur_sec == 0 and counter != 0:\n                start = time.time()\n                feats_refit = feat[-STOCK_NUM*SEC_NUM-len(test_filled):-len(test_filled)].drop('not_pad', axis=1)\n                target_refit = target[-STOCK_NUM*SEC_NUM-len(test_filled):-len(test_filled)]\n                for i in range(len(models)):\n                    models[i] = models[i].refit(feats_refit, target_refit['target'], decay_rate=online_train_decay)\n                print(f\"refit time = {time.time() - start}\")\n        \n        feat = feat[-len(test_filled):]\n        feat = feat[feat['not_pad'] == 1].drop('not_pad', axis=1)\n        \n        # Generate predictions for each model and calculate the weighted average\n        inf_start = time.time()\n        preds_list = []\n        for model in models:\n            preds_list.append(model.predict(feat))\n        predictions = sum(preds_list) / len(preds_list)  \n        predictions = predictions - np.mean(predictions)\n        predictions = np.clip(predictions, y_min, y_max)\n        sample_prediction['target'] = predictions\n        inf_time.append(time.time() - inf_start)\n        env.predict(sample_prediction)\n        \n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps), 'gen_time', np.mean(gen_time), 'inf_time', np.mean(inf_time))","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:22:17.077673Z","iopub.execute_input":"2023-12-20T18:22:17.077983Z","iopub.status.idle":"2023-12-20T18:25:50.063477Z","shell.execute_reply.started":"2023-12-20T18:22:17.077954Z","shell.execute_reply":"2023-12-20T18:25:50.062650Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n10 qps: 1.1259600400924683 gen_time 0.8338542222976685 inf_time 0.2704096794128418\n20 qps: 1.1199040174484254 gen_time 0.8176470041275025 inf_time 0.28276400566101073\n30 qps: 1.1096340894699097 gen_time 0.8121590296427409 inf_time 0.27860652605692543\n40 qps: 1.1073296129703523 gen_time 0.8116705775260925 inf_time 0.2771796226501465\n50 qps: 1.116520504951477 gen_time 0.8154175806045533 inf_time 0.28273979663848875\n60 qps: 1.1320046504338583 gen_time 0.8248473962148031 inf_time 0.28766753276189166\n70 qps: 1.1481600965772356 gen_time 0.8372050421578544 inf_time 0.29161362988608225\n80 qps: 1.1591553866863251 gen_time 0.8505742192268372 inf_time 0.2893630117177963\n90 qps: 1.1673168314827813 gen_time 0.8605035543441772 inf_time 0.2877385007010566\n100 qps: 1.1852417945861817 gen_time 0.8727592468261719 inf_time 0.29342265367507936\n110 qps: 1.2088516322049228 gen_time 0.8848616903478449 inf_time 0.30488061254674736\n120 qps: 1.2270338277022044 gen_time 0.9018298427263896 inf_time 0.3053198834260305\n130 qps: 1.23973996089055 gen_time 0.9170239797005286 inf_time 0.3029123673072228\n140 qps: 1.2545630523136684 gen_time 0.9319623163768224 inf_time 0.30282210963112965\n150 qps: 1.266037041346232 gen_time 0.945231728553772 inf_time 0.3010754203796387\n160 qps: 1.279076512157917 gen_time 0.9584031954407692 inf_time 0.3009455993771553\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}